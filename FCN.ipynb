{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+0mn4C94vU/+XUQ8al9Gj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engineerJPark/FCN_Implementation/blob/main/FCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 크기 어떻게 될지 살펴볼 것\n",
        "\n",
        "\n",
        "https://github.com/wkentaro/pytorch-fcn/blob/ad22a47671140b2abc6712a0e004193fa382bc48/torchfcn/models/fcn16s.py#L78\n",
        "\n",
        "dataset 참고할 것\n",
        "\n",
        "https://pytorch.org/vision/stable/generated/torchvision.datasets.VOCSegmentation.html#torchvision.datasets.VOCSegmentation\n",
        "\n",
        "COCO dataset\n",
        "\n",
        "https://ndb796.tistory.com/667\n",
        "\n",
        "Label Accuracy Code\n",
        "https://github.com/wkentaro/pytorch-fcn/blob/main/torchfcn/utils.py\n",
        "\n",
        "Loss and Train Code\n",
        "https://github.com/wkentaro/pytorch-fcn/blob/main/torchfcn/trainer.py\n"
      ],
      "metadata": {
        "id": "9UfbPl0sNqoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test zone\n",
        "# inputs = torch.randn((3,3,16,16))\n",
        "# targets = torch.randint(3,(3,16,16), dtype=torch.float)\n",
        "# y = F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
        "# y\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "# x.repeat(4, 2)\n",
        "x.repeat(4, 2, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z8hUpHynT2n",
        "outputId": "e4d2ac25-11c9-44f6-def5-a6b2db5274a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [1, 2, 3]],\n",
              "\n",
              "        [[1, 2, 3],\n",
              "         [1, 2, 3]],\n",
              "\n",
              "        [[1, 2, 3],\n",
              "         [1, 2, 3]],\n",
              "\n",
              "        [[1, 2, 3],\n",
              "         [1, 2, 3]]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6jBAYOT35FG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "resnet_pretrained = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "print(resnet_pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # dataloader on COCO dataset\n",
        "\n",
        "# # use name as train_set, val_set, test_set\n",
        "\n",
        "# # need to be fixed\n",
        "# transform = transforms.Compose([\n",
        "#                 transforms.ToTensor(),\n",
        "#                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "#             ])\n",
        "\n",
        "# cifar10_train = dset.CIFAR10('./datasets', train=True, download=True,\n",
        "#                              transform=transform)\n",
        "# loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "#                           sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "# cifar10_val = dset.CIFAR10('./datasets', train=True, download=True,\n",
        "#                            transform=transform)\n",
        "# loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "#                         sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "# cifar10_test = dset.CIFAR10('./datasets', train=False, download=True, \n",
        "#                             transform=transform)\n",
        "# loader_test = DataLoader(cifar10_test, batch_size=64)\n",
        "\n",
        "# Nin, Cin, Hin, Win, class_n = voc_train.shape"
      ],
      "metadata": {
        "id": "CTG7oSo4uxJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bilinear weights deconvolution Algorithm 알아보기\n",
        "def bilinear_kernel_init(Cin, Cout, kernel_size):\n",
        "  factor = (kernel_size + 1) // 2\n",
        "  if kernel_size % 2 == 1:\n",
        "    center = (kernel_size + 1) // 2 - 1\n",
        "  else:\n",
        "    center = (kernel_size + 1) // 2 - 0.5\n",
        "\n",
        "  og = (torch.arange(kernel_size).reshape(-1,1), torch.arange(kernel_size).reshape(1,-1))\n",
        "  filter = (1 - torch.abs(og[0] - center) / factor) * (1 - torch.abs(og[1] - center) / factor)\n",
        "\n",
        "  weight = torch.zeros((Cin, Cout, kernel_size, kernel_size))\n",
        "  weight[range(Cin), range(Cout), :, :] = filter\n",
        "  return weight\n"
      ],
      "metadata": {
        "id": "C-iNBBNGc1lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCN18(nn.Module):\n",
        "  def __init__(self, class_n):\n",
        "    super().__init__()\n",
        "    self.downsample1 = nn.Sequential( # 1/2 of the input\n",
        "        resnet_pretrained.conv1,\n",
        "        resnet_pretrained.bn1,\n",
        "        resnet_pretrained.relu,\n",
        "        resnet_pretrained.maxpool\n",
        "    )\n",
        "    self.downsample2 = resnet_pretrained.layer1 # 1/4 of the input\n",
        "    self.downsample3 = resnet_pretrained.layer2 # 1/8 of the input, 여기서 1x1 거친것 하나 추출\n",
        "    self.downsample4 = resnet_pretrained.layer3 # 1/16 of the input, 여기서 1x1 거친것 하나 추출\n",
        "    self.downsample5 = resnet_pretrained.layer4 # 1/32 of the input, 여기서 1x1 거친것 하나 추출\n",
        "\n",
        "    self.fully_conv_pool3 = nn.Conv2d(128, class_n, kernel_size=1) # get class score for each pixel\n",
        "    nn.init.xavier_normal_(self.fully_conv_pool3.weight)\n",
        "    # nn.init.xavier_normal_(self.fully_conv_pool3.bias)\n",
        "\n",
        "    self.fully_conv_pool4 = nn.Conv2d(256, class_n, kernel_size=1) # get class score for each pixel\n",
        "    nn.init.xavier_normal_(self.fully_conv_pool4.weight)\n",
        "    # nn.init.xavier_normal_(self.fully_conv_pool4.bias)\n",
        "\n",
        "    self.fully_conv_pool5 = nn.Conv2d(512, class_n, kernel_size=1) # get class score for each pixel\n",
        "    nn.init.xavier_normal_(self.fully_conv_pool5.weight)\n",
        "    # nn.init.xavier_normal_(self.fully_conv_pool5.bias)\n",
        "    \n",
        "    # 보통 stride s, padding s/2, kernelsize 2s -> s배 만큼 이미지가 커진다.\n",
        "    # 이부분 튜닝해야한다.\n",
        "    self.upsample_make_16s = nn.ConvTranspose2d(class_n, class_n, kernel_size=4, padding=1, stride=2, bias=False) # to 1/16\n",
        "    self.upsample_make_16s.weight.data.copy_(bilinear_kernel_init(class_n, class_n, 4))\n",
        "\n",
        "    self.upsample_make_8s = nn.ConvTranspose2d(class_n, class_n, kernel_size=4, padding=1, stride=2, bias=False) # to 1/8\n",
        "    self.upsample_make_8s.weight.data.copy_(bilinear_kernel_init(class_n, class_n, 4))\n",
        "    \n",
        "    self.upsample_to_score = nn.ConvTranspose2d(class_n, class_n, kernel_size=16, padding=4, stride=8) # to 1\n",
        "    self.upsample_to_score.weight.data.copy_(bilinear_kernel_init(class_n, class_n, 16))\n",
        "    for param in self.upsample_to_score.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.downsample1(x)\n",
        "    x = self.downsample2(x)\n",
        "    x = self.downsample3(x)\n",
        "    pool3_score = self.fully_conv_pool3(x)\n",
        "    x = self.downsample4(x)\n",
        "    pool4_score = self.fully_conv_pool4(x)\n",
        "    x = self.downsample5(x)\n",
        "    pool5_score = self.fully_conv_pool5(x)\n",
        "\n",
        "    pool4_2x_conv7 = pool4_score + self.upsample_make_16s(pool5_score)\n",
        "    pool3_2x_pool4_4x_conv7 = pool3_score + self.upsample_make_8s(pool4_2x_conv7)\n",
        "\n",
        "    out = self.upsample_to_score(pool3_2x_pool4_4x_conv7)\n",
        "\n",
        "    return out\n",
        "\n",
        "# model = FCN18(3)\n",
        "# x = torch.rand([10, 3,1024,1024])\n",
        "# print(model(x).shape)"
      ],
      "metadata": {
        "id": "SSp7WMlbCOdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "lr = 10e-4\n",
        "weight_decay = 2e-4\n",
        "momentum = 0.9\n",
        "batch_size = 20\n",
        "\n",
        "model = FCN18(3)\n",
        "optimizer = optim.SGD(model.paramters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "def loss_func(self, inputs, targets):\n",
        "  '''\n",
        "  cross entropy to each feature vector\n",
        "  inputs shape : (N, C, H, W)\n",
        "  targets shape : (N, C, H, W)\n",
        "  만약 tragets가 (N, H, W)이라면 : target.view(n, h, w, 1).repeat(1, 1, 1, c)\n",
        "  '''\n",
        "  feature_vector = F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1) # output a feature vector : shape is (channel, )\n",
        "  return feature_vector.mean()\n",
        "\n",
        "def train():\n",
        "  for epoch in range(epochs):\n",
        "    for train_img, train_gt_img in train_set:\n",
        "      score_img = model(train_img)\n",
        "      loss = loss_func(score_img, train_gt_img)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      optimizer.step()\n",
        "      loss.backward()\n",
        "    print(\"epoch %d, loss : %f \"%(epoch + 1, loss))\n",
        "  print(\"Training End\")"
      ],
      "metadata": {
        "id": "duCh3xvTs8rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "  '''\n",
        "  implement mean IU first \n",
        "  '''\n",
        "  pass\n",
        "\n",
        "\n",
        "def validate(self):\n",
        "    training = self.model.training\n",
        "    self.model.eval()\n",
        "\n",
        "    n_class = len(self.val_loader.dataset.class_names)\n",
        "\n",
        "    val_loss = 0\n",
        "    visualizations = []\n",
        "    label_trues, label_preds = [], []\n",
        "    for batch_idx, (data, target) in tqdm.tqdm(\n",
        "            enumerate(self.val_loader), total=len(self.val_loader),\n",
        "            desc='Valid iteration=%d' % self.iteration, ncols=80,\n",
        "            leave=False):\n",
        "        if self.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        with torch.no_grad():\n",
        "            score = self.model(data)\n",
        "\n",
        "        loss = cross_entropy2d(score, target,\n",
        "                                size_average=self.size_average)\n",
        "        loss_data = loss.data.item()\n",
        "        if np.isnan(loss_data):\n",
        "            raise ValueError('loss is nan while validating')\n",
        "        val_loss += loss_data / len(data)\n",
        "\n",
        "        imgs = data.data.cpu()\n",
        "        lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
        "        lbl_true = target.data.cpu()\n",
        "        for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n",
        "            img, lt = self.val_loader.dataset.untransform(img, lt)\n",
        "            label_trues.append(lt)\n",
        "            label_preds.append(lp)\n",
        "            if len(visualizations) < 9:\n",
        "                viz = fcn.utils.visualize_segmentation(\n",
        "                    lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class)\n",
        "                visualizations.append(viz)\n",
        "    metrics = torchfcn.utils.label_accuracy_score(\n",
        "        label_trues, label_preds, n_class)\n",
        "\n",
        "    out = osp.join(self.out, 'visualization_viz')\n",
        "    if not osp.exists(out):\n",
        "        os.makedirs(out)\n",
        "    out_file = osp.join(out, 'iter%012d.jpg' % self.iteration)\n",
        "    skimage.io.imsave(out_file, fcn.utils.get_tile_image(visualizations))\n",
        "\n",
        "    val_loss /= len(self.val_loader)\n",
        "\n",
        "    with open(osp.join(self.out, 'log.csv'), 'a') as f:\n",
        "        elapsed_time = (\n",
        "            datetime.datetime.now(pytz.timezone('Asia/Tokyo')) -\n",
        "            self.timestamp_start).total_seconds()\n",
        "        log = [self.epoch, self.iteration] + [''] * 5 + \\\n",
        "              [val_loss] + list(metrics) + [elapsed_time]\n",
        "        log = map(str, log)\n",
        "        f.write(','.join(log) + '\\n')\n",
        "\n",
        "    mean_iu = metrics[2]\n",
        "    is_best = mean_iu > self.best_mean_iu\n",
        "    if is_best:\n",
        "        self.best_mean_iu = mean_iu\n",
        "    torch.save({\n",
        "        'epoch': self.epoch,\n",
        "        'iteration': self.iteration,\n",
        "        'arch': self.model.__class__.__name__,\n",
        "        'optim_state_dict': self.optim.state_dict(),\n",
        "        'model_state_dict': self.model.state_dict(),\n",
        "        'best_mean_iu': self.best_mean_iu,\n",
        "    }, osp.join(self.out, 'checkpoint.pth.tar'))\n",
        "    if is_best:\n",
        "        shutil.copy(osp.join(self.out, 'checkpoint.pth.tar'),\n",
        "                    osp.join(self.out, 'model_best.pth.tar'))\n",
        "\n",
        "    if training:\n",
        "        self.model.train()"
      ],
      "metadata": {
        "id": "eWH2odR-kDJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}